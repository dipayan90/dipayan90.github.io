{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Connect Senior Engineer at Nordstrom Inc email: dipayan90@gmail.com blog: dipayan90.wordpress.com linkedin: www.linkedin.com/in/dchattopadhyay github: github.com/dipayan90 Download CV","title":"Connect"},{"location":"#connect","text":"Senior Engineer at Nordstrom Inc email: dipayan90@gmail.com blog: dipayan90.wordpress.com linkedin: www.linkedin.com/in/dchattopadhyay github: github.com/dipayan90 Download CV","title":"Connect"},{"location":"experience/","text":"Experience Nordstrom Senior Engineer - ML Platform Dates Employed: Nov 2018 \u2013 Present Location: Greater Seattle Area Enable the data scientists in the organization conduct frequent ML experiments. Provide means for rapid prototyping of chosen models. Build a platform that is scalable, allows for reproducibility and is very flexible in terms of language or frameworks being chosen. Provision compute resources in the form of Apache spark applications or kubernetes pods for all steps in the ML model development life cycle. Architect this platform, establish proof of concepts and work with stakeholders to figure out what missing pieces still need to be embedded as part of the platform. This platform currently supports end to end pipeline for 10+ ML projects involving productionizing of 15+ models. Talk on this platform by me can be found here: youtube video Senior Data Engineer Dates Employed: Feb 2018 \u2013 Nov 2018 Location: Greater Seattle Area Write processes to ingest massive amounts of data from different sources filesystems ( HDFS, S3 ) , Streams ( Kinesis, Kafka ) , Data warehouses ( Redshift ) into a data-lake solution ( currently S3 ). Write ETLs to pre-process data for ML needs. ETLs are primarily done using Apache Spark, however other tools like Apache NiFi and AWS Lambda are also used. Lot of focus is put on making the data pipelines robust and have lots of instrumentation done for alerting and metrics. Views are created using Apache Hive/ Presto for data stored on data lake. Infrastructure is provisioned via Terraform. Apache Airflow is used to create Dags which are executed on a regular cadence. Notable project: Created a pipeline to ingest and process 1 million records every 30 secs. Engineer 2 Dates Employed: Feb 2017 \u2013 Feb 2018 Location: Greater Seattle Area Part of the customer profile services ( CPS ) team, where we source data and expose public facing APIs for actual end customers. Responsible for setting up a spark cluster and running spark jobs to move over large datasets ( around 0.5 B records ) to operational datastore (DynamoDB) in a lossless timely fashion introducing fault tolerance and failure retry mechanisms. Responsible for setting up consumers to consume data from AWS Kinesis streams, curate and persist them in near real time fashion. Responsible for creating a Rest Based Public API involving token based authentication and authorization using Amazon API Gateway, Amazon Cognito and AWS lambda. Set up health monitoring for all the involved systems using AWS lambda and integrated pagerduty and slack using amazon SNS. Architecting enterprise applications using serverless technologies leading to cost optimizations, better monitoring and auditing and championing separation of concerns. Set Up artillery scripts to performance test APIs and publish results to grafana dashboard. CDK Global ( Formerly ADP ) Software Engineer 2 Dates Employed: Jul 2015 \u2013 Feb 2017 Location: Greater Seattle Area Part of audience management team where we gauge user intent and provide a seamless personalized experience. Most of my experience has been on architecting several data driven business intelligence reports that visualize KPIs and provide actionable insights and correlations to customers. Full stack developer responsible for creation of several Java/Spring based micro services following practice of domain driven design. Architected NodeJs based webapps that use Angular 1.X as the JS framework for developing single page applications. Developed several big data pipelines, involving both batch and streaming based processing. Depending on the volume of data we use Kafka and Rabbit MQ message brokers for our messaging needs. Have been playing around lately with Apache Spark and Akka for our stream processing needs. Mentored junior developers and interns. Provided technical guidance to offshore teams and worked as scrum master. Part of Docker tribe where we help teams across company run their applications on docker containers and run containers on CoreOS grid using Consul for service discovery. Software Engineer 1 Dates Employed: Aug 2014 \u2013 Jun 2015 Location: Greater Seattle Area Part of social media management team, where we provide a consolidated social media management platform to more than 26,000 car dealerships. Most of my work included aggregating social media feeds from different channels using several exposed REST APIs. Full stack developer responsible for creating several RESTful web services using J2EE/Spring and being one of the key contributors towards creation of a company wide Node- Angular web application stack. Promoted from software engineer-1 (August 2014 \u2013 July 2015) in record 11 months, faster than any other associate with a similar role. UPS Software Development Intern Dates Employed: Aug 2013 \u2013 Jan 2014 Location: Alpharetta GA Developed applications for integration middleware platform where modular applications were being developed as part of the OSGI architecture. Worked on JBOSS Fuse ESB, Camel Mediation Router, Apache Active-MQ, Restful Web Services and XSLT Transforms. Worked with business analysts to understand business needs and requirement specifications. Collaborated with multiple teams to agree on contracts and created Interface agreement documents. Designed and documented architectural flows with the help of system architects. As one of the first members to join the development team, took a lot of initiatives to kick start projects and later helped other members to get on board. University of Cincinnati Technical Consultant Dates Employed: Feb 2013 \u2013 Jun 2013 Location: Cincinnati, Ohio Area Maintaining several computing systems all across the University. Troubleshooting software,OS and Network related issues. Providing technical support to faculty and staff. TitleTeaching Assistant Dates Employed: Aug 2012 \u2013 Feb 2013 Location: Cincinnati Assisted freshmen student in the introductory and advanced class on Matlab. Supervised and graded class on basic engineering foundations. Assisted students with assignments in tutoring center.","title":"Experience"},{"location":"experience/#experience","text":"","title":"Experience"},{"location":"experience/#nordstrom","text":"","title":"Nordstrom"},{"location":"experience/#senior-engineer-ml-platform","text":"Dates Employed: Nov 2018 \u2013 Present Location: Greater Seattle Area Enable the data scientists in the organization conduct frequent ML experiments. Provide means for rapid prototyping of chosen models. Build a platform that is scalable, allows for reproducibility and is very flexible in terms of language or frameworks being chosen. Provision compute resources in the form of Apache spark applications or kubernetes pods for all steps in the ML model development life cycle. Architect this platform, establish proof of concepts and work with stakeholders to figure out what missing pieces still need to be embedded as part of the platform. This platform currently supports end to end pipeline for 10+ ML projects involving productionizing of 15+ models. Talk on this platform by me can be found here: youtube video","title":"Senior Engineer - ML Platform"},{"location":"experience/#senior-data-engineer","text":"Dates Employed: Feb 2018 \u2013 Nov 2018 Location: Greater Seattle Area Write processes to ingest massive amounts of data from different sources filesystems ( HDFS, S3 ) , Streams ( Kinesis, Kafka ) , Data warehouses ( Redshift ) into a data-lake solution ( currently S3 ). Write ETLs to pre-process data for ML needs. ETLs are primarily done using Apache Spark, however other tools like Apache NiFi and AWS Lambda are also used. Lot of focus is put on making the data pipelines robust and have lots of instrumentation done for alerting and metrics. Views are created using Apache Hive/ Presto for data stored on data lake. Infrastructure is provisioned via Terraform. Apache Airflow is used to create Dags which are executed on a regular cadence. Notable project: Created a pipeline to ingest and process 1 million records every 30 secs.","title":"Senior Data Engineer"},{"location":"experience/#engineer-2","text":"Dates Employed: Feb 2017 \u2013 Feb 2018 Location: Greater Seattle Area Part of the customer profile services ( CPS ) team, where we source data and expose public facing APIs for actual end customers. Responsible for setting up a spark cluster and running spark jobs to move over large datasets ( around 0.5 B records ) to operational datastore (DynamoDB) in a lossless timely fashion introducing fault tolerance and failure retry mechanisms. Responsible for setting up consumers to consume data from AWS Kinesis streams, curate and persist them in near real time fashion. Responsible for creating a Rest Based Public API involving token based authentication and authorization using Amazon API Gateway, Amazon Cognito and AWS lambda. Set up health monitoring for all the involved systems using AWS lambda and integrated pagerduty and slack using amazon SNS. Architecting enterprise applications using serverless technologies leading to cost optimizations, better monitoring and auditing and championing separation of concerns. Set Up artillery scripts to performance test APIs and publish results to grafana dashboard.","title":"Engineer 2"},{"location":"experience/#cdk-global-formerly-adp","text":"","title":"CDK Global ( Formerly ADP )"},{"location":"experience/#software-engineer-2","text":"Dates Employed: Jul 2015 \u2013 Feb 2017 Location: Greater Seattle Area Part of audience management team where we gauge user intent and provide a seamless personalized experience. Most of my experience has been on architecting several data driven business intelligence reports that visualize KPIs and provide actionable insights and correlations to customers. Full stack developer responsible for creation of several Java/Spring based micro services following practice of domain driven design. Architected NodeJs based webapps that use Angular 1.X as the JS framework for developing single page applications. Developed several big data pipelines, involving both batch and streaming based processing. Depending on the volume of data we use Kafka and Rabbit MQ message brokers for our messaging needs. Have been playing around lately with Apache Spark and Akka for our stream processing needs. Mentored junior developers and interns. Provided technical guidance to offshore teams and worked as scrum master. Part of Docker tribe where we help teams across company run their applications on docker containers and run containers on CoreOS grid using Consul for service discovery.","title":"Software Engineer 2"},{"location":"experience/#software-engineer-1","text":"Dates Employed: Aug 2014 \u2013 Jun 2015 Location: Greater Seattle Area Part of social media management team, where we provide a consolidated social media management platform to more than 26,000 car dealerships. Most of my work included aggregating social media feeds from different channels using several exposed REST APIs. Full stack developer responsible for creating several RESTful web services using J2EE/Spring and being one of the key contributors towards creation of a company wide Node- Angular web application stack. Promoted from software engineer-1 (August 2014 \u2013 July 2015) in record 11 months, faster than any other associate with a similar role.","title":"Software Engineer 1"},{"location":"experience/#ups","text":"","title":"UPS"},{"location":"experience/#software-development-intern","text":"Dates Employed: Aug 2013 \u2013 Jan 2014 Location: Alpharetta GA Developed applications for integration middleware platform where modular applications were being developed as part of the OSGI architecture. Worked on JBOSS Fuse ESB, Camel Mediation Router, Apache Active-MQ, Restful Web Services and XSLT Transforms. Worked with business analysts to understand business needs and requirement specifications. Collaborated with multiple teams to agree on contracts and created Interface agreement documents. Designed and documented architectural flows with the help of system architects. As one of the first members to join the development team, took a lot of initiatives to kick start projects and later helped other members to get on board.","title":"Software Development Intern"},{"location":"experience/#university-of-cincinnati","text":"","title":"University of Cincinnati"},{"location":"experience/#technical-consultant","text":"Dates Employed: Feb 2013 \u2013 Jun 2013 Location: Cincinnati, Ohio Area Maintaining several computing systems all across the University. Troubleshooting software,OS and Network related issues. Providing technical support to faculty and staff.","title":"Technical Consultant"},{"location":"experience/#titleteaching-assistant","text":"Dates Employed: Aug 2012 \u2013 Feb 2013 Location: Cincinnati Assisted freshmen student in the introductory and advanced class on Matlab. Supervised and graded class on basic engineering foundations. Assisted students with assignments in tutoring center.","title":"TitleTeaching Assistant"},{"location":"miscellaneous/","text":"Miscellaneous Member of IEEE graduate student body for Cincinnati chapter. I can speak 5 different languages. Sports enthusiast, regularly play tennis, racquetball and ping pong. Have won numerous ping-pong competitions. Active member of a nonprofit organization VIBHA Seattle, that aims to improve the lives of poor children by spreading awareness and providing the tools necessary for educating them.","title":"Miscellaneous"},{"location":"miscellaneous/#miscellaneous","text":"Member of IEEE graduate student body for Cincinnati chapter. I can speak 5 different languages. Sports enthusiast, regularly play tennis, racquetball and ping pong. Have won numerous ping-pong competitions. Active member of a nonprofit organization VIBHA Seattle, that aims to improve the lives of poor children by spreading awareness and providing the tools necessary for educating them.","title":"Miscellaneous"},{"location":"opensource/","text":"Opensource projects Spring actuator cloudwatch Big-data Profiler Deep Learning Image Similarity Autoerroretry Time-Series based anomaly detection","title":"OpenSource"},{"location":"opensource/#opensource-projects","text":"Spring actuator cloudwatch Big-data Profiler Deep Learning Image Similarity Autoerroretry Time-Series based anomaly detection","title":"Opensource projects"},{"location":"skills/","text":"Skills Name Types LANGUAGES JAVA, PYTHON, SCALA, JAVASCRIPT FRAMEWORKS SPRING FRAMEWORK, NODEJS, EXPRESS JS, APACHE CAMEL, REDHAT FUSE UI FRAMEWORKS REACT, VUE.JS, ANGULARJS 1.X, BOOTSTRAP DATABASES MONGO DB, REDIS, MY SQL, ORACLE DEVOPS TOOLS DOCKER, KUBERNETES, GITLAB, COREOS, CONSUL, NGINX, ATLASSIAN BAMBOO, JENKINS, ELK STACK BIGDATA FRAMEWORKS APACHE SPARK, APACHE HIVE, APACHE NIFI, APACHE AIRFLOW DATA ANALYTICS TOOLS MATLAB, ORANGE, TABLEAU MESSAGE BROKERS RABBIT MQ, ACTIVE MQ, KAFKA AWS SERVICES AWS SERVICES USED IAM, S3, LAMBDA, DYNAMODB, KINESIS, FIREHOSE, API GATEWAY, EC2","title":"Skills"},{"location":"skills/#skills","text":"Name Types LANGUAGES JAVA, PYTHON, SCALA, JAVASCRIPT FRAMEWORKS SPRING FRAMEWORK, NODEJS, EXPRESS JS, APACHE CAMEL, REDHAT FUSE UI FRAMEWORKS REACT, VUE.JS, ANGULARJS 1.X, BOOTSTRAP DATABASES MONGO DB, REDIS, MY SQL, ORACLE DEVOPS TOOLS DOCKER, KUBERNETES, GITLAB, COREOS, CONSUL, NGINX, ATLASSIAN BAMBOO, JENKINS, ELK STACK BIGDATA FRAMEWORKS APACHE SPARK, APACHE HIVE, APACHE NIFI, APACHE AIRFLOW DATA ANALYTICS TOOLS MATLAB, ORANGE, TABLEAU MESSAGE BROKERS RABBIT MQ, ACTIVE MQ, KAFKA AWS SERVICES AWS SERVICES USED IAM, S3, LAMBDA, DYNAMODB, KINESIS, FIREHOSE, API GATEWAY, EC2","title":"Skills"},{"location":"summary/","text":"Summary MS in computer science from University of Cincinnati ( Thesis on Data mining ) advised by Professor Raj Bhatnagar. Passionate about developments in data science research Working on creating large scale ETL operations on Big Data using Apache Spark, Apache Hive/Apache Tez. Working experience on scheduling applications using Apache Airflow. Experience creating near realtime data processing flows on Apache Kafka or AWS Kinesis. Experience provisioning infrastructure using Terraform. Working experience on Java and related technologies like Spring MVC, Spring Social, Spring security etc. Have developed RESTful webservices from scratch. Have developed front end application based on Angular Js and related technologies like Grunt, Jasmine, Protractor, karma etc. Working knowledge of nodeJs server and routing frameworks like express. Have experience working with Apache Camel, JBoss Fuse, Spring Framework, OSGI services, Apache Active MQ, Maven. Familiar with Eclipse, IntelliJ, Webstorm,Netbeans , XML Spy , Matlab, SVN, Git and Perforce.","title":"Introduction"},{"location":"summary/#summary","text":"MS in computer science from University of Cincinnati ( Thesis on Data mining ) advised by Professor Raj Bhatnagar. Passionate about developments in data science research Working on creating large scale ETL operations on Big Data using Apache Spark, Apache Hive/Apache Tez. Working experience on scheduling applications using Apache Airflow. Experience creating near realtime data processing flows on Apache Kafka or AWS Kinesis. Experience provisioning infrastructure using Terraform. Working experience on Java and related technologies like Spring MVC, Spring Social, Spring security etc. Have developed RESTful webservices from scratch. Have developed front end application based on Angular Js and related technologies like Grunt, Jasmine, Protractor, karma etc. Working knowledge of nodeJs server and routing frameworks like express. Have experience working with Apache Camel, JBoss Fuse, Spring Framework, OSGI services, Apache Active MQ, Maven. Familiar with Eclipse, IntelliJ, Webstorm,Netbeans , XML Spy , Matlab, SVN, Git and Perforce.","title":"Summary"}]}